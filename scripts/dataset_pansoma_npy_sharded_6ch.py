import os
import glob
from typing import List, Tuple, Dict, Union
from collections import OrderedDict

import numpy as np
import torch
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader, ConcatDataset

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Sharded NPY dataset with LRU memmap cache (fixes open-FD blowup)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NPYShardDataset(Dataset):
    """
    Dataset for sharded .npy files written as:

        shard_x: (N, 6, H, W)   e.g.  shard_train_000_x.npy
        shard_y: (N,)           e.g.  shard_train_000_y.npy

    This version:
      â€¢ Does NOT keep all shards memmapped at once (avoids 'Too many open files').
      â€¢ Only stores paths + sizes in __init__.
      â€¢ Lazily opens shards with np.load(..., mmap_mode="r") in __getitem__.
      â€¢ Uses a small LRU cache (max_cached_shards) per worker to reuse open memmaps.
    """

    def __init__(
        self,
        root_dir: str,
        transform=None,
        return_paths: bool = False,
        max_cached_shards: int = 2,   # <= tune: RAM vs # of open files
    ):
        self.root_dir = os.path.abspath(os.path.expanduser(root_dir))
        self.transform = transform
        self.return_paths = return_paths
        self.max_cached_shards = max_cached_shards

        if not os.path.isdir(self.root_dir):
            raise FileNotFoundError(f"Root directory not found: {self.root_dir}")

        # Expect files like:  shard_xxx_x.npy  &  shard_xxx_y.npy
        x_paths = sorted(glob.glob(os.path.join(self.root_dir, "*_x.npy")))
        y_paths = sorted(glob.glob(os.path.join(self.root_dir, "*_y.npy")))

        if not x_paths:
            raise ValueError(f"No *_x.npy shard files found in {self.root_dir}")
        if len(x_paths) != len(y_paths):
            raise ValueError(
                f"Shard count mismatch in {self.root_dir}: "
                f"{len(x_paths)} *_x.npy vs {len(y_paths)} *_y.npy"
            )

        self.x_paths: List[str] = x_paths
        self.y_paths: List[str] = y_paths
        self.shard_sizes: List[int] = []
        self.cumulative_sizes: List[int] = []

        self.C = None
        self.H = None
        self.W = None
        total = 0

        # â”€â”€ Scan each shard once to get N and shape, then immediately close â”€â”€
        for x_path, y_path in zip(self.x_paths, self.y_paths):
            # x: (N, 6, H, W) int8 (memmapped; only header is read)
            x_arr = np.load(x_path, mmap_mode="r")
            if x_arr.ndim != 4 or x_arr.shape[1] != 6:
                raise ValueError(
                    f"{x_path}: expected x shape (N, 6, H, W), got {x_arr.shape}"
                )

            n, C, H, W = x_arr.shape
            if self.C is None:
                self.C, self.H, self.W = C, H, W
            else:
                if (C, H, W) != (self.C, self.H, self.W):
                    raise ValueError(
                        f"{x_path}: inconsistent shard shape {x_arr.shape}, "
                        f"expected (*, {self.C}, {self.H}, {self.W})"
                    )
            # drop the memmap -> closes FD
            del x_arr

            # y: (N,)
            y_arr = np.load(y_path, mmap_mode="r")
            if y_arr.shape[0] != n:
                raise ValueError(
                    f"{y_path}: label length mismatch: {y_arr.shape[0]} vs {n}"
                )
            del y_arr

            self.shard_sizes.append(n)
            total += n
            self.cumulative_sizes.append(total)

        if total == 0:
            raise ValueError(
                f"NPYShardDataset from {self.root_dir} has zero usable samples."
            )

        # LRU cache: shard_idx -> (x_memmap, y_memmap)
        self._shard_cache: "OrderedDict[int, Tuple[np.memmap, np.memmap]]" = OrderedDict()

        print(
            f"Initialized NPYShardDataset from {self.root_dir}: "
            f"{len(self.x_paths)} shards, {total} samples total. "
            f"shape per sample = (6, {self.H}, {self.W}), "
            f"max_cached_shards={self.max_cached_shards}"
        )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  indexing helpers
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def __len__(self) -> int:
        return self.cumulative_sizes[-1]

    def _locate(self, idx: int) -> Tuple[int, int]:
        """Global index -> (shard_idx, local_idx)."""
        if idx < 0 or idx >= len(self):
            raise IndexError(f"Index {idx} out of range 0..{len(self)-1}")

        # linear scan is OK for few hundred shards; can binary-search if needed
        for shard_idx, cum in enumerate(self.cumulative_sizes):
            if idx < cum:
                prev_cum = 0 if shard_idx == 0 else self.cumulative_sizes[shard_idx - 1]
                local_idx = idx - prev_cum
                return shard_idx, local_idx

        raise RuntimeError(f"Failed to locate index {idx}")

    def _get_shard_arrays(self, shard_idx: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Return (x_memmap, y_memmap) for a given shard_idx.

        Uses an LRU cache to limit how many shards are currently memmapped,
        which avoids blowing past the OS file-descriptor limit.
        """
        # Cache hit: move to end (most recently used)
        if shard_idx in self._shard_cache:
            x_arr, y_arr = self._shard_cache.pop(shard_idx)
            self._shard_cache[shard_idx] = (x_arr, y_arr)
            return x_arr, y_arr

        # Cache miss: open with memmap
        x_path = self.x_paths[shard_idx]
        y_path = self.y_paths[shard_idx]

        x_arr = np.load(x_path, mmap_mode="r")   # (N, 6, H, W)
        y_arr = np.load(y_path, mmap_mode="r")   # (N,)

        # store in cache
        self._shard_cache[shard_idx] = (x_arr, y_arr)

        # Evict least-recently-used shard if over capacity
        if len(self._shard_cache) > self.max_cached_shards:
            old_idx, (old_x, old_y) = self._shard_cache.popitem(last=False)
            # dropping references allows OS to close FDs
            del old_x
            del old_y

        return x_arr, y_arr

    def __getitem__(self, idx: int):
        shard_idx, local_idx = self._locate(idx)
        x_arr, y_arr = self._get_shard_arrays(shard_idx)

        try:
            x = x_arr[local_idx]   # (6, H, W) view into memmap
            y = y_arr[local_idx]   # scalar
        except Exception as e:
            raise RuntimeError(
                f"Failed to access local_idx={local_idx} in shard_idx={shard_idx}: {e}"
            )

        if x.ndim != 3 or x.shape[0] != 6:
            raise ValueError(
                f"Sample from shard_idx={shard_idx} local_idx={local_idx} "
                f"has shape {x.shape}, expected (6, H, W)."
            )

        # Make a writable, contiguous copy in float32 (cheap compared to disk I/O & GPU compute)
        x_np = np.array(x, dtype=np.float32, copy=True)  # ensures writable
        x_tensor = torch.from_numpy(x_np)

        y_tensor = torch.tensor(int(y), dtype=torch.long)

        if self.transform is not None:
            x_tensor = self.transform(x_tensor)

        if self.return_paths:
            sample_id = f"{os.path.basename(self.x_paths[shard_idx])}#{local_idx}"
            return x_tensor, y_tensor, sample_id
        else:
            return x_tensor, y_tensor


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Genotype map & get_data_loader (unchanged API)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GENOTYPE_MAP: Dict[str, int] = {
    "false": 0,
    "true": 1,
}

def _to_list(x):
    if x is None:
        return []
    if isinstance(x, (list, tuple)):
        return list(x)
    return [x]

def get_data_loader(
    data_dir: Union[str, List[str], Tuple],
    dataset_type: str,
    batch_size: int = 32,
    num_workers: int = 16,
    shuffle: bool = False,
    return_paths: bool = False,
):
    """
    NPY-sharded version of your original get_data_loader, same interface.

    Layout per root:
        root/
          train/
            shard_train_000_x.npy
            shard_train_000_y.npy
            ...
          val/
            shard_val_000_x.npy
            shard_val_000_y.npy
            ...

    Supports:
      â€¢ data_dir = "/path/to/root"
      â€¢ data_dir = ["/root1", "/root2", ...]
      â€¢ data_dir = (train_roots, val_roots)  # split-mode
    """
    # Decide roots & subfolders
    if (
        isinstance(data_dir, (list, tuple))
        and len(data_dir) == 2
        and (isinstance(data_dir[0], (str, list, tuple)) and isinstance(data_dir[1], (str, list, tuple)))
    ):
        roots = _to_list(data_dir[0] if dataset_type == "train" else data_dir[1])
        subfolders = ["train", "val"]  # your original slightly-unusual behavior
    else:
        roots = _to_list(data_dir)
        subfolders = [dataset_type]

    dataset_dirs: List[str] = []
    for r in roots:
        r = os.path.abspath(os.path.expanduser(r))
        for sf in subfolders:
            dataset_dirs.append(os.path.join(r, sf))

    missing = [p for p in dataset_dirs if not os.path.exists(p)]
    if missing:
        raise FileNotFoundError(f"Dataset path(s) do not exist: {missing}")

    # Same 6-channel normalization as before
    transform = transforms.Compose([
        transforms.Normalize(
            mean=[
                18.417816162109375,
                12.649129867553711,
                -0.5452527403831482,
                24.723854064941406,
                4.690611362457275,
                0.2813551473402196,
            ],
            std=[
                25.028322219848633,
                14.809632301330566,
                0.6181337833404541,
                29.972835540771484,
                7.9231791496276855,
                0.7659083659074717,
            ],
        )
    ])

    datasets: List[Dataset] = []
    for d in dataset_dirs:
        ds = NPYShardDataset(
            root_dir=d,
            transform=transform,
            return_paths=return_paths,
            max_cached_shards=256,   # <- keep this small to avoid too many open files
        )
        datasets.append(ds)

    if len(datasets) == 1:
        dataset = datasets[0]
    else:
        dataset = ConcatDataset(datasets)

    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
    )

    return loader, GENOTYPE_MAP


# (Optional) simple main for quick smoke test
if __name__ == "__main__":
    data_root = "/path/to/your_6channel_npy_sharded_dataset"  # contains train/ and val/
    batch_size = 16
    num_workers = 0

    if data_root == "/path/to/your_6channel_npy_sharded_dataset":
        print("ðŸ›‘ Please update 'data_root' to your NPY-sharded dataset root (with train/ and val/).")
    else:
        try:
            print(f"\n--- Loading Training Data (Batch Size: {batch_size}) ---")
            train_loader, class_map = get_data_loader(
                data_dir=data_root,
                dataset_type="train",
                batch_size=batch_size,
                num_workers=num_workers,
                shuffle=True,
            )
            print(f"âœ… Loaded {len(train_loader.dataset)} training samples.")
            print(f"Genotype map: {class_map}")

            for i, (data, labels) in enumerate(train_loader):
                print(f"Batch {i + 1}: data.shape={data.shape}, labels[0:5]={labels[:5]}")
                if i >= 1:
                    break

        except Exception as e:
            print(f"Error loading training data: {e}")
